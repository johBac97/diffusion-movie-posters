{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "import lightning as L\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn, utils\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_root = Path.cwd().parent / \"data\" / \"MNIST\"\n",
    "\n",
    "data_root_path = (\n",
    "    Path.cwd() / \"data\" / \"four_genre_posters_updated\" / \"four_genre_posters_updated\"\n",
    ")\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# dataset = ImageFolder(data_root_path, transform=transform)\n",
    "\n",
    "dataset = MNIST(mnist_root, download=True, transform=transform)\n",
    "\n",
    "if isinstance(dataset, ImageFolder):\n",
    "\n",
    "    class_to_idx = dataset.find_classes(data_root_path)[0]\n",
    "elif isinstance(dataset, MNIST):\n",
    "    class_to_idx = {x: x for x in range(0, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x710e54741610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, target = dataset[4]\n",
    "print(target)\n",
    "\n",
    "plt.imshow(image.permute((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosterGeneratorNetwork(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        class_embedding_size: int,\n",
    "        num_classes: int,\n",
    "        num_timesteps: int,\n",
    "    ):\n",
    "        super(PosterGeneratorNetwork, self).__init__()\n",
    "\n",
    "        self._image_size = image_size\n",
    "        self._num_timesteps = num_timesteps\n",
    "\n",
    "        self._class_embedding = nn.Embedding(num_classes, class_embedding_size)\n",
    "\n",
    "        self._model = UNet2DModel(\n",
    "            sample_size=image_size,\n",
    "            in_channels=3 + class_embedding_size,\n",
    "            block_out_channels=(32, 64, 64),\n",
    "            down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "            up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "        )\n",
    "\n",
    "        self._noise_scheduler = DDPMScheduler(\n",
    "            num_train_timesteps=self._num_timesteps, beta_schedule=\"squaredcos_cap_v2\"\n",
    "        )\n",
    "\n",
    "        self._loss_fcn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, t, class_idx):\n",
    "\n",
    "        class_conditioning = self._class_embedding(class_idx)\n",
    "\n",
    "        class_conditioning = class_conditioning.reshape(\n",
    "            (*class_conditioning.shape, 1, 1)\n",
    "        ).expand((*class_conditioning.shape, *x.shape[2:]))\n",
    "\n",
    "        image_input = torch.cat((x, class_conditioning), dim=1)\n",
    "\n",
    "        return self._model(image_input, t).sample\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        images, labels = batch\n",
    "\n",
    "        noise = torch.randn_like(images).to(self.device)\n",
    "\n",
    "        timesteps = (\n",
    "            torch.randint(0, self._num_timesteps - 1, (images.shape[0],))\n",
    "            .long()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        noisy_x = self._noise_scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        pred = self(noisy_x, timesteps, labels)\n",
    "\n",
    "        loss = self._loss_fcn(pred, noise)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def generate_images(self, labels: torch.Tensor | list):\n",
    "        if isinstance(labels, list):\n",
    "            labels = torch.Tensor(labels)\n",
    "\n",
    "        labels = labels.long().to(self.device)\n",
    "\n",
    "        x = torch.randn(labels.shape[0], 3, self._image_size, self._image_size).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        for indx, t in enumerate(self._noise_scheduler.timesteps):\n",
    "            with torch.no_grad():\n",
    "                residual = self(x, t, labels)\n",
    "\n",
    "            x = self._noise_scheduler.step(residual, t, x).prev_sample\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class_embedding_size = 4\n",
    "num_timesteps = 20\n",
    "\n",
    "task = PosterGeneratorNetwork(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    class_embedding_size=class_embedding_size,\n",
    "    num_classes=len(class_to_idx),\n",
    "    num_timesteps=num_timesteps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule, Trainer\n",
    "\n",
    "\n",
    "def plot_images(images, labels):\n",
    "    fig, ax = plt.subplots(images.shape[0], figsize=(16, 16))\n",
    "\n",
    "    images = images.detach().cpu()\n",
    "\n",
    "    images -= image.min(dim=2, keepdim=True).values.min(dim=3, keepdim=True).values\n",
    "    images /= image.max(dim=2, keepdim=True).values.max(dim=3, keepdim=True).values\n",
    "\n",
    "    for image, ax, label in zip(images, ax, labels):\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.set_title(label)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "class GenerateImagesCallback(L.Callback):\n",
    "    def __init__(self, every_n_epoch: int):\n",
    "        super(GenerateImagesCallback, self).__init__()\n",
    "        self._every_n_epoch = every_n_epoch\n",
    "        self._counter = 0\n",
    "\n",
    "        self._labels = [0, 1, 2, 3]\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n",
    "        self._counter += 1\n",
    "\n",
    "        if self._counter % self._every_n_epoch == 0:\n",
    "            # Create image\n",
    "            images = pl_module.generate_images(self._labels)\n",
    "\n",
    "            fig = plot_images(images, self._labels)\n",
    "\n",
    "            images_dir = Path(pl_module.logger.log_dir) / \"images\"\n",
    "            images_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            fig.savefig(images_dir / f\"images_{self._counter}.png\")\n",
    "\n",
    "\n",
    "generate_images_callback = GenerateImagesCallback(every_n_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dataloader = utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    min_epochs=1,\n",
    "    max_epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type        | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | _class_embedding | Embedding   | 16     | train\n",
      "1 | _model           | UNet2DModel | 274 M  | train\n",
      "2 | _loss_fcn        | MSELoss     | 0      | train\n",
      "---------------------------------------------------------\n",
      "274 M     Trainable params\n",
      "0         Non-trainable params\n",
      "274 M     Total params\n",
      "1,096.257 Total estimated model params size (MB)\n",
      "354       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/john/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969646d2fe7541febbf9e3c3fdfb731b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 24.38 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 75.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    210\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     adam(\n\u001b[1;32m    224\u001b[0m         params_with_grad,\n\u001b[1;32m    225\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mp/diffusion-movie-posters/env/lib/python3.11/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    153\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    154\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    163\u001b[0m         p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    164\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 24.38 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 75.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.fit(task, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
